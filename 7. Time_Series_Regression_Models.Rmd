---
title: "7. Time_Series_Regression_Models.RMD"
author: "Russ Conte"
date: "10/17/2021"
output: html_document
---

In this chapter we discuss regression models. The basic concept is that we forecast the time series of interest y assuming that it has a linear relationship with other time series $x$.

For example, we might wish to forecast monthly sales $y$ using total advertising spend $x$ as a predictor. Or we might forecast daily electricity demand y using temperature $x_1$ and the day of week $x_2$ as predictors.

The <b>forecast variable</b> $y$ is sometimes also called the regressand, dependent or explained variable. The <b>predictor variables</b> $x$ are sometimes also called the regressors, independent or explanatory variables. <mark><b>In this book we will always refer to them as the “forecast” variable and “predictor” variables.</b></mark>

## 7.1 The Linear Model

In the simplest case, the regression model allows for a linear relationship between the forecast variable y and a single predictor variable $x$:

$$y_t = \beta_0 + \beta_1x_t+\epsilon_t$$

An artificial example of data from such a model is shown in Figure 7.1. The coefficients $\beta_0$ and $\beta_1$ denote the intercept and the slope of the line respectively. The intercept $\beta_0$ represents the predicted value of $y$ when $x$=0. The slope $\beta_1$ represents the average predicted change in $y$ resulting from a one unit increase in $x$.

```{r}
library(tidyverse)
library(fpp3)
ggplot(data = mtcars,mapping = aes(x = wt, y = mpg)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  ggtitle(label = 'Wt vs mpg for MTCars')

```

## Example: US Consumption Expenditure

Figure 7.2 shows time series of quarterly percentage changes (growth rates) of real personal consumption expenditure, $y$, and real personal disposable income, $x$, for the US from 1970 Q1 to 2019 Q2.

```{r Figure 7.2: Percentage changes in personal consumption expenditure and personal income for the US.}

us_change %>% 
  pivot_longer(c(Consumption, Income), names_to = "Series") %>% 
  autoplot(value) +
  labs(y = "% change")

View(us_change)

```

A scatter plot of the consumption changes against income changes is showin in Figure 7.3, along with the estimated regression line:

$$\hat{y_t} = 0.54 + 0.27x_t$$

```{r Figure 7.3: Scatterplot of quarterly changes in consumption expenditure versus quarterly changes in personal income and the fitted regression line.}

us_change %>% 
  ggplot(aes(x = Income, y = Consumption)) +
  labs(y = "Consumption (quarterly % change)",
       x = "Income (quarterly % change)") +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)

```

The equation is estimated using the `TSLM()` (time series liner model) function:

```{r Time series linear model applied to US_change}
us_change %>% 
  model(TSLM(Consumption ~ Income)) %>% 
  report()
```

## Multiple Linear Regression

When there are two or more predictor variables, the model is called a multiple regression model. The general form of a multiple regression model is

$$y_t=\beta_0 + \beta_1 x_{1,t} + \beta_2 x_{2,t} + ... + \beta_k x_{k, t} + \epsilon_t$$

where $y$ is the variable to be forecast and $x_1,…,x_k$ are the $k$ predictor variables. Each of the predictor variables must be numerical. The coefficients $\beta_1,...\beta_k$ measure the effect of each predictor after taking into account the effects of all the other predictors in the model. Thus, the coefficients measure the marginal effects of the predictor variables.

## Example: US Consumption Expenditure

Figure 7.4 shows additional predictors that may be useful for forecasting US consumption expenditure. These are quarterly percentage changes in industrial production and personal savings, and quarterly changes in the unemployment rate (as this is already a percentage). Building a multiple linear regression model can potentially generate more accurate forecasts as we expect consumption expenditure to not only depend on personal income but on other predictors as well.

```{r}
# Answered at: https://stackoverflow.com/questions/69606489/plotting-each-variable-in-a-time-series-on-the-same-plot/69606629#69606629
library(tidyverse)
us_change %>%
  pivot_longer(-Quarter) %>%
  ggplot(aes(Quarter, value, color = name)) +
  geom_line() +
  facet_wrap(~name, ncol = 1, scales = "free_y") +
  guides(color = "none")


```

Figure 7.5 is a scatterplot matrix of five variables. The first column shows the relationships between the forecast variable (consumption) and each of the predictors. The scatterplots show positive relationships with income and industrial production, and negative relationships with savings and unemployment. The strength of these relationships are shown by the correlation coefficients across the first row. The remaining scatterplots and correlation coefficients show the relationships between the predictors.

```{r Figure 7.5: A scatterplot matrix of US consumption expenditure and the four predictors. }

us_change %>% 
  GGally::ggpairs(columns = 2:6)

```

## Assumptions

When we use a linear regression model, we are implicitly making some assumptions about the variables in Equation (7.1).

First, we assume that the model is a reasonable approximation to reality; that is, the relationship between the forecast variable and the predictor variables satisfies this linear equation.

Second, we make the following assumptions about the errors $( \epsilon_1...\epsilon_T)$

•   they have mean zero; otherwise the forecasts will be systematically biased.
•   they are not autocorrelated; otherwise the forecasts will be inefficient, as there is more information in the data that can be exploited.
•    they are unrelated to the predictor variables; otherwise there would be more information that should be included in the systematic part of the model.

It is also useful to have the errors being normally distributed with a constant variance $\sigma^2$

in order to easily produce prediction intervals.

Another important assumption in the linear regression model is that each predictor $x$
is not a random variable. If we were performing a controlled experiment in a laboratory, we could control the values of each $x$ (so they would not be random) and observe the resulting values of $y$. With observational data (including most data in business and economics), it is not possible to control the value of $x$, we simply observe it. Hence we make this an assumption.

## 7.2 Least Squares Estimation

In practice, of course, we have a collection of observations but we do not know the values of the coefficients $\beta_0, \beta_1, ... \beta_k$. These need to be estimated from the data.

The least squares principle provides a way of choosing the coefficients effectively by minimising the sum of the squared errors. That is, we choose the values of $\beta_0, \beta_1, ... \beta_k$ that minimise

$\sum_{t=1}^T \epsilon_t^2 = \sum_{t=1}^T (y_t - \beta_0 - \beta_1 x_{1,t} - \beta_2 x_{2,t} - ... - \beta_k x_{k, t})^2$

This is called <b>least squares</b> estimation because it gives the least value for the sum of squared errors. Finding the best estimates of the coefficients is often called “fitting” the model to the data, or sometimes “learning” or “training” the model. The line shown in Figure 7.3 was obtained in this way.

The `TSLM()` function fits a linear regression model to time series data. It is similar to the `lm()` function which is widely used for linear models, but `TSLM()` provides additional facilities for handling time series.

## Example: US Consumption Expenditure

A multiple linear regression model for US consumption is:

$y_t = \beta_0 + \beta_1 x_{1,t} + \beta_2 x_{2,t}  + \beta_3 x_{3,t} + \beta_4 x_{4,t} +  \epsilon_t$

where $y$ is the percentage change in real personal consumption expenditure, $x_1$ is the percentage change in real personal disposable income, $x_2$ is the percentage change in industrial production, $x_3$ is the percentage change in personal savings and $x_4$ is the change in the unemployment rate.

The following output provides information about the fitted model. The first column of `Coefficients` gives an estimate of each $\beta$ coefficient and the second column gives its standard error (i.e., the standard deviation which would be obtained from repeatedly estimating the $\beta$ coefficients on similar data sets). The standard error gives a measure of the uncertainty in the estimated $\beta$ coefficient.

```{r Information about the fitted model}

fit_consMR <- us_change %>% 
  model(tslm = TSLM((Consumption ~ Income + Production + Unemployment + Savings)))

report(fit_consMR)

```

For forecasting purposes, the final two columns are of limited interest. The “t value” is the ratio of an estimated $\beta$ coefficient to its standard error and the last column gives the p-value: the probability of the estimated $\beta$ coefficient being as large as it is if there was no real relationship between consumption and the corresponding predictor. <b>This is useful when studying the effect of each predictor, but is not particularly useful for forecasting.</b>

## Fitted Values

Predictions of y can be obtained by using the estimated coefficients in the regression equation and setting the error term to zero. In general we write:

$$\hat{y} = \hat{\beta_0} + \hat{\beta_1 x_{1,t}} +\hat{\beta_2}x_{2,t} + ... + \hat{\beta_k} x_{k,t} $$
Plugging in the values of $x_{1,t}, ... ,x_{k,t}$ for $t$=1,…,T returns predictions of $y_t$ within the training set, referred to as fitted values. Note that these are predictions of the data used to estimate the model, not genuine forecasts of future values of $y$.

The following plots show the actual values compared to the fitted values for the percentage change in the US consumption expenditure series. The time plot in Figure 7.6 shows that the fitted values follow the actual data fairly closely. This is verified by the strong positive relationship shown by the scatterplot in Figure 7.7.

```{r Figure 7.6: Time plot of actual US consumption expenditure and predicted US consumption expenditure.}

augment(fit_consMR) %>% 
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Consumption, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = NULL,
       title = "Percentage change in US consumption expenditure") +
  scale_color_manual(values = c(Data = "black", Fitted = "#D55E00")) +
  guides(colour = guide_legend(title = "Legend"))

```

```{r Figure 7.7: Actual US consumption expenditure plotted against predicted US consumption expenditure.}

augment(fit_consMR) %>% 
  ggplot(aes(x = Consumption, y = .fitted)) +
  geom_point() +
  labs(
    y = "Fitted (predicted values)",
    x = "Data (actual values)",
    title = "Percent change in US consumption expendutire"
  ) +
  geom_abline(intercept = 0, slope = 1)

```

## Goodness-of-Fit

A common way to summarise how well a linear regression model fits the data is via the coefficient of determination, or $R^2$. This can be calculated as the square of the correlation between the observed $y$ values and the predicted $\hat{y}$ values. Alternatively, it can also be calculated as,

$$R^2 = \frac{\sum(\hat{y}_{t} - \bar{y})^2}{\sum(y_{t}-\bar{y})^2},$$

where the summations are over all observations. Thus, it reflects the proportion of variation in the forecast variable that is accounted for (or explained) by the regression model.

In simple linear regression, the value of $R^2$is also equal to the square of the correlation between 
$y$ and $X$ (provided an intercept has been included).

If the predictions are close to the actual values, we would expect $R^2$ to be close to 1. On the other hand, if the predictions are unrelated to the actual values, then $R^2$=0 (again, assuming there is an intercept). In all cases, $R^2$ lies between 0 and 1.

The $R^2$ value is used frequently, though often incorrectly, in forecasting. The value of $R^2$ will never decrease when adding an extra predictor to the model and this can lead to over-fitting. There are no set rules for what is a good $R^2$ value, and typical values of $R^2$ depend on the type of data used. Validating a model’s forecasting performance on the test data is much better than measuring the $R^2$ value on the training data.

Figure 7.7 plots the actual consumption expenditure values versus the fitted values. The correlation between these variables is $r$=0.877 hence $R^2$=0.768 (shown in the output above). In this case model does an excellent job as it explains 76.8% of the variation in the consumption data. Compare that to the $R^2$ value of 0.15 obtained from the simple regression with the same data set in Section 7.1. Adding the three extra predictors has allowed a lot more of the variation in the consumption data to be explained.

## Standard Error of the Regression

Another measure of how well the model has fitted the data is the standard deviation of the residuals, which is often known as the “residual standard error.” This is shown in the above output with the value 0.31.

It is calculated using:

$\hat{\sigma_e} = \sqrt {\frac{1}{T - k - 1}\sum_{t = 1}^T e_t^2}$

where k is the number of predictors in the model. Notice that we divide by T−k−1 because we have estimated k+1 parameters (the intercept and a coefficient for each predictor variable) in computing the residuals.

The standard error is related to the size of the average error that the model produces. We can compare this error to the sample mean of y or with the standard deviation of y

to gain some perspective on the accuracy of the model.

The standard error will be used when generating prediction intervals, discussed in Section 7.6.

# 7.3 Evaulating the regression model

The differences between the observed y values and the corresponding fitted ^y values are the training-set errors or “residuals” defined as:

$ e_t = y_t - \hat{y_t}$
$\text{   } = y_t - \hat{\beta_1{x_{1,t}}} - \hat{\beta_2{x_{2,t}}} - ... - \hat{\beta_k{x_{k,t}}}$

for $t = 1,...T.$ Each residual is the unpredictable component of the associated observation.

As a result of these properties, it is clear that the average of the residuals is zero, and that the correlation between the residuals and the observations for the predictor variable is also zero. (This is not necessarily true when the intercept is omitted from the model.)

## ACF plot of residuclas

With time series data, it is highly likely that the value of a variable observed in the current time period will be similar to its value in the previous period, or even the period before that, and so on. Therefore when fitting a regression model to time series data, it is common to find autocorrelation in the residuals. In this case, the estimated model violates the assumption of no autocorrelation in the errors, and our forecasts may be inefficient — there is some information left over which should be accounted for in the model in order to obtain better forecasts. The forecasts from a model with autocorrelated errors are still unbiased, and so are not “wrong,” but they will usually have larger prediction intervals than they need to. Therefore we should always look at an ACF plot of the residuals.

## Histogram of residuals

It is always a good idea to check whether the residuals are normally distributed. As we explained earlier, this is not essential for forecasting, but it does make the calculation of prediction intervals much easier.

### Example
using the `gg_tsresiduals()` function introduced in Section 5.3, we can obtain all the useful residual diagnostics mentioned above:

```{r  Figure 7.8: Analysing the residuals from a regression model for US quarterly consumption.}

fit_consMR %>% gg_tsresiduals()

```

```{r}

augment(fit_consMR) %>% 
  features(.innov, ljung_box, lag = 10, dof = 5)
```

The time plot shows some changing variation over time, but is otherwise relatively unremarkable. This heteroscedasticity will potentially make the prediction interval coverage inaccurate.

The histogram shows that the residuals seem to be slightly skewed, which may also affect the coverage probability of the prediction intervals.

## Residual plot against predictors


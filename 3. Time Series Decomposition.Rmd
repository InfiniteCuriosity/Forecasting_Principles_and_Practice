---
title: "3. Time Series Decomposition.RMD"
author: "Russ Conte"
date: "10/3/2021"
output: html_document
---
---
title: "3. Time Series Decomposition.RMD"
author: "Russ Conte"
date: "10/3/2021"
output: html_document
---

Adjusting the historical data can often lead to a simpler time series. Here, we deal with four kinds of adjustments: calendar adjustments, population adjustments, inflation adjustments and mathematical transformations. The purpose of these adjustments and transformations is to simplify the patterns in the historical data by removing known sources of variation, or by making the pattern more consistent across the whole data set. Simpler patterns are usually easier to model and lead to more accurate forecasts.

<span style="color:#0000ff;">Calendar adjustments</span><br>

Some of the variation seen in seasonal data may be due to simple calendar effects. In such cases, it is usually much easier to remove the variation before doing any further analysis.

For example, if you are studying the total monthly sales in a retail store, there will be variation between the months simply because of the different numbers of trading days in each month, in addition to the seasonal variation across the year. It is easy to remove this variation by computing average sales per trading day in each month, rather than total sales in the month. Then we effectively remove the calendar variation.

<span style="color:#0000ff;">Population adjustments</span><br>

Any data that are affected by population changes can be adjusted to give per-capita data. That is, consider the data per person (or per thousand people, or per million people) rather than the total. For example, if you are studying the number of hospital beds in a particular region over time, the results are much easier to interpret if you remove the effects of population changes by considering the number of beds per thousand people. Then you can see whether there have been real increases in the number of beds, or whether the increases are due entirely to population increases. It is possible for the total number of beds to increase, but the number of beds per thousand people to decrease. This occurs when the population is increasing faster than the number of hospital beds. <b>For most data that are affected by population changes, it is best to use per-capita data rather than the totals.</b>

```{r Example of population adjustments in Australia}
library(tidyverse)
library(fpp3)
global_economy %>% 
  filter(Country == "Australia") %>% 
  autoplot(GDP/Population) +
  labs(title = "GDP per capita", y = "$US")
```

<span style="color:#0000ff;">Inflation adjustments</span><br>

<b>Data which are affected by the value of money are best adjusted before modelling.</b> For example, the average cost of a new house will have increased over the last few decades due to inflation. A $200,000 house this year is not the same as a $200,000 house twenty years ago. For this reason, <b>financial time series are usually adjusted so that all values are stated in dollar values from a particular year.</b> For example, the house price data may be stated in year 2000 dollars.

To make these adjustments, a price index is used. If $$z_t \textrm{ denotes the price index and } y_t \textrm{ denotes the house price in year t, then } x_t = \frac{y_t}{z_t * z_2000}$$ gives the adjusted house price at year 2000 dollar values. Price indexes are often constructed by government agencies. For consumer goods, a common price index is the Consumer Price Index (or CPI).

```{r Inflation adjustments example}

print_retail <- aus_retail %>% 
  filter(Industry == "Newspaper and book retailing") %>% 
  group_by(Industry) %>% 
  index_by(Year = year(Month)) %>% 
  summarise(Turnover = sum(Turnover))

aus_economy <- global_economy %>% 
  filter(Code == "AUS")

print_retail %>% 
  left_join(aus_economy, by = "Year") %>% 
  mutate(Adjusted_turnover = Turnover / CPI * 100) %>% 
  pivot_longer(c(Turnover, Adjusted_turnover), values_to = "Turnover") %>% 
  mutate(name = factor(name, levels = c("Turnover", "Adjusted_turnover"))) %>% 
  ggplot(aes(x = Year, y = Turnover)) +
  geom_line() +
  facet_grid(name~., scales = "free_y") +
  labs(title = "Turnover: Australian print media industry", y = "$AUS")
```

By adjusting for inflation using the CPI, we can see that Australia’s newspaper and book retailing industry has been in decline much longer than the original data suggests. The adjusted turnover is in 2010 Australian dollars, as CPI is 100 in 2010 in this data set.

<span style="color:#0000ff;">Mathematical transformations</span><br>

If the data shows variation that increases or decreases with the level of the series, then a transformation can be useful. For example, a logarithmic transformation is often useful.

Sometimes other transformations are also used (although they are not so interpretable). For example, square roots and cube roots can be used. These are called power transformations because they can be written in the form $$w_t = y^p_t$$

A useful family of transformations, that includes both logarithms and power transformations, is the family of <b>Box-Cox transformations</b> (Box & Cox, 1964), which depend on the parameter λ and are defined as follows:

$$w_t = \begin{cases}
 log(y_t) \textrm{ if } \lambda  = 0;\\ 
 (sign(y_t)|y_t|^\lambda -1)/\lambda  \textrm{ otherwise} 
\end{cases}$$

A good value of λ is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler. In this case, λ = 0.10 works quite well, although any value of λ between 0.0 and 0.2 would give similar results.
  
The `guerrero` feature (Guerrero, 1993) can be used to choose a value of lambda for you. In this case it chooses λ = 0.12.

```{r Example of the Guerrero feature to apply Box-Cox transformation to a time serie}
lambda <- aus_production %>% 
  features(Gas, features = guerrero) %>% 
  pull(lambda_guerrero)

aus_production %>% 
  autoplot(box_cox(Gas, lambda)) +
  labs(y = "",
       title = latex2exp::TeX(paste0("Transformed gas production with $\\lambda$ = ",
                                    round(lambda, 2))))
```

## 3.2 Time series components

If we assume an additive decomposition, then we can write:

$$y_t = S_t + T_t + R_t$$
where $$y_t \textrm{ is the data, } S_t \textrm{ is the seasonal component, } T_t\textrm{ is the trend-cycle component, }  and R_t \textrm{ is the remainder component at a time t.}$$ Alternatively, a multiplicative decomposition would be written as:

$$y_t = S_t \times T_t \times R_t$$

## Employment in the US Retail Sector

We will decompose the number of persons employed in retail as shown in Figure 3.5. The data shows the total monthly number of persons in thousands employed in the retail sector across the US since 1990.

```{r Total number of persons employed in US retail.}
us_retail_employment <- us_employment %>%
  filter(year(Month) >= 1990, Title == "Retail Trade") %>% 
  select(-Series_ID)
autoplot(us_retail_employment, Employed) +
  labs(y = "Persons (thousands)",
       title = "Total employment in US Retail")

```

To illustrate the ideas, we will use the STL decomposition method, which is discussed in Section 3.6.

```{r Example of STL decomposition}
dcmp <- us_retail_employment %>% 
  model(stl = STL(Employed))
components(dcmp)

```

The output above shows the components of an STL decomposition. The original data is shown (as `Employed`), followed by the estimated components. This output forms a “dable” or decomposition table. The header to the table shows that the `Employed` series has been decomposed additively.

The `trend` column containing the trend-cycle $$T_t \textrm{ follows the overall movement of the series, ignoring any seasonality and random fluctuations, as shown in Figure 3.6.}$$
```{r Figure 3.6: Total number of persons employed in US retail: the trend-cycle component (orange) and the raw data (grey).}
components(dcmp) %>% 
  as_tsibble() %>% 
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y = trend), colour = "#D55E00") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US Retail")
```

We can plot all of the components in a single figure using `autoplot()`, as shown in Figure 3.7.

```{r Plot all of the components in one figure}
components(dcmp) %>% autoplot()
```

## Seasonally adjusted data

If the seasonal component is removed from the original data, the resulting values are the “seasonally adjusted” data. For an additive decomposition, the seasonally adjusted data are given by $$y_t - S_t$$ and for multiplicative data, the seasonally adjusted values are obtained using $$y_t/S_t$$

```{r Figure 3.8, Seasonally adjusted data}
components(dcmp) %>%
  as_tsibble() %>%
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y = season_adjust), colour = "#0072B2") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US Retail")

```

If the variation due to seasonality is not of primary interest, the seasonally adjusted series can be useful. For example, monthly unemployment data are usually seasonally adjusted in order to highlight variation due to the underlying state of the economy rather than the seasonal variation. An increase in unemployment due to school leavers seeking work is seasonal variation, while an increase in unemployment due to an economic recession is non-seasonal. Most economic analysts who study unemployment data are more interested in the non-seasonal variation. Consequently, employment data (and many other economic series) are usually seasonally adjusted.

## 3.3 Moving Averages

The first step in a classical decomposition is to use a moving average method to estimate the trend-cycle, so we begin by discussing moving averages.

<span style="color:#0000ff;">Moving average smoothing</span><br>

The moving average of order `m` can be written as:

$$\hat{T_t} = \frac{1}{m}\sum_{j = -k}^{k}y_{t+j}$$

where m = 2k+1

Observations that are nearby in time are also likely to be close in value. Therefore, the average eliminates some of the randomness in the data, leaving a smooth trend-cycle component. We call this an m<b>-MA</b>, meaning a moving average of order m.

```{r Figure 3.9: Australian exports of goods and services: 1960–2017.}
global_economy %>% 
  filter(Country == "Australia") %>% 
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Annual Total Australian Exports")

```

```{r}
global_economy %>% 
  filter(Country == "Australia") %>% 
  select(Year, Exports)
```

In the last column of this table, a moving average of order 5 is shown, providing an estimate of the trend-cycle. The first value in this column is the average of the first five observations, 1960–1964; the second value in the 5-MA column is the average of the values for 1961–1965; and so on. This is easily computed using `slide_dbl()` from the `slider` package which applies a function to “sliding” time windows. In this case, we use the mean() function with a window of size 5.

```{r 5-year moving average}
aus_exports <- global_economy %>% 
   filter(Country == "Australia") %>% 
   mutate(
     `5-MA` = slider::slide2_dbl(Exports, mean,
                  .before = 2,  .after = 2, .complete = TRUE))
aus_exports
```

To see what the trend-cycle estimate looks like, we plot it along with the original data in Figure 3.10.

```{r Figure 3.10: Australian exports (black) along with the 5-MA estimate of the trend-cycle (orange).}
aus_exports %>% 
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports, 5-year moving average") +
  guides(colour = guide_legend(title = "series"))



```

Notice that the trend-cycle (in orange) is smoother than the original data and captures the main movement of the time series without all of the minor fluctuations. The order of the moving average determines the smoothness of the trend-cycle estimate. In general, a larger order means a smoother curve.

## Moving averages of moving averages

It is possible to apply a moving average to a moving average. One reason for doing this is to make an even-order moving average symmetric.

For example, we might take a moving average of order 4, and then apply another moving average of order 2 to the results. In the following table, this has been done for the first few years of the Australian quarterly beer production data.

```{r Table 3.2: A moving average of order 4 applied to the quarterly beer data, followed by a moving average of order 2.}

beer <- aus_production %>% 
  filter(year(Quarter) >= 1992) %>% 
  select(Quarter, Beer)

beer_ma <-  beer %>%
  mutate(
    `4-MA` = slider::slide_dbl(Beer, mean,
                                .before = 1, .after = 2, .complete = TRUE),
    `2x4-MA` = slider::slide_dbl(`4-MA`, mean,
                                  .before = 1, .after = 0, .complete = TRUE)
  )
beer_ma

```

The notation “ 2×4-MA” in the last column means a 4-MA followed by a 2-MA. The values in the last column are obtained by taking a moving average of order 2 of the values in the previous column. For example, the first two values in the 4-MA column are 451.25=(443+410+420+532)/4 and 448.75=(410+420+532+433)/4. The first value in the 2x4-MA column is the average of these two: 450.00=(451.25+448.75)/2.

When a 2-MA follows a moving average of an even order (such as 4), it is called a “centred moving average of order 4.” This is because the results are now symmetric. To see that this is the case, we can write the 2×4-MA as follows:

$$\hat{T_t} = \frac{1}{2}\left [\frac{1}{4}(y_{t-2} + y_{t-1} +y_t + y_{t+1}) + \frac{1}{4}(y_{t-1} + y_{t} +y_{t+1} + y_{t+2})\right]\\	
= \frac{1}{8}y_{t-2} + \frac{1}{4}y_{t-1} + \frac{1}{4}y_{t+1} + \frac{1}{8}y_{t+2}$$

## Estimating the trend = cycle with seasonal data

The most common use of centered moving averages is for estimating the trend = cycl from seasonal data. Consider the 2x4-MA:

$$\hat{T_t} = \frac{1}{8}y_{t-2} + \frac{1}{4}y_{t-1} + \frac{1}{4}y_{t+1} + \frac{1}{8}y_{t+2}$$

When applied to quarterly data, each quarter of the year is given equal weight as the first and last terms apply to the same quarter in consecutive years. Consequently, the seasonal variation will be averaged out and the resulting values of $$\hat{T_t}$$ will have little or no seasonal variation remaining. A similar effect wuld be obtained using a 2xz8-MA or 2x12-MA to quarterly data.

## Example: Employment in the US Retail Sector

```{r Figure 3.12: A 2x12-MA applied to the US retail employment series.}

us_retail_employment_ma <- us_retail_employment %>% 
  mutate(
    `12-MA` = slider::slide_dbl(Employed, mean,
                                .before = 5, .after = 6, .complete = TRUE),
    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,
                                  .before = 1, .after = 0, .complete = TRUE)
  )

us_retail_employment_ma %>% 
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y = `2x12-MA`), colour = "#D55E00") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US Retail")
```

## 3.4 Classical decomposition

### Additive decomposition

<span style="color:#0000ff;">Step 1:</span><br>

$$\textrm{If m is an even number, compute the trend-cycle component }\hat{T_t} \textrm{ using a 2 x MA. If m is an odd number, compute the trebd-cycle component}\hat{T_t} \textrm{using an m=MA.}$$

<span style="color:#0000ff;">Step 2:</span><br>
Calculate the detrended series: $$y_t - \hat{T_t}$$

<span style="color:#0000ff;">Step 3:</span><br>
To estimate the seasonal component for each season, simply average the detrended values for that season. For example, with monthly data, the seasonal component for March is the average of all the detrended March values in the data. These seasonal component values are then adjusted to ensure that they add to zero. The seasonal component is obtained by stringing together these monthly values, and then replicating the sequence for each year of data. This gives $$\hat{S_t}$$

<span style="color:#0000ff;">Step 4:</span><br>

The remainder component is calculated by subracting the estimated seasonal and trend-cycle components: $$\hat{R_t} = \hat{y_t} - \hat{T_t} - \hat{S_t}$$

Figure 3.13 shows a classical decomposition of the total retail employment series across the US.

```{r Figure 3.13: A classical additive decomposition of US retail employment.}
us_retail_employment %>% 
  model(
    classical_decomposition(Employed, type = "additive")
  ) %>% 
  components() %>% 
  autoplot() +
  labs(title = "Classical additive decomposition of total US retail employment")
```

### Comments of classical decomposition

While classical decomposition is still widely used, it is not recommended, as there are now several much better methods. Some of the problems with classical decomposition are summarised below.

• The estimate of the trend-cycle is unavailable for the first few and last few observations. For example, if m=12, there is no trend-cycle estimate for the first six or the last six observations. Consequently, there is also no estimate of the remainder component for the same time periods.
• The trend-cycle estimate tends to over-smooth rapid rises and falls in the data.
• Classical decomposition methods assume that the seasonal component repeats from year to year. For many series, this is a reasonable assumption, but for some longer series it is not. For example, electricity demand patterns have changed over time as air conditioning has become more widespread. In many locations, the seasonal usage pattern from several decades ago had its maximum demand in winter (due to heating), while the current seasonal pattern has its maximum demand in summer (due to air conditioning). Classical decomposition methods are unable to capture these seasonal changes over time.
• Occasionally, the values of the time series in a small number of periods may be particularly unusual. For example, the monthly air passenger traffic may be affected by an industrial dispute, making the traffic during the dispute different from usual. The classical method is not robust to these kinds of unusual values.


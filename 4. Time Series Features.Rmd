---
title: "4. Time Series Features.RMD"
author: "Russ Conte"
date: "10/3/2021"
output: html_document
---

The `feasts` package includes functions for computing FEatures And Statistics from Time Series (hence the name).

We can compute many different features on many different time series, and use them to explore the properties of the series. In this chapter we will look at some features that have been found useful in time series exploration, and how they can be used to uncover interesting information about your data. We will use Australian quarterly tourism as a running example.

## 4.1 Some simple statistics

Any numerical summary computed from a time series is a feature of that time series — the mean, minimum or maximum, for example. These can be computed using the features() function. For example, let’s compute the means of all the series in the Australian tourism data.

```{r Compute the mean of visits for all the series for the Australian tourism data}
library(fpp3)
library(tidyverse)
tourism %>% 
  features(Trips, list(mean = mean)) %>% 
  arrange(desc(mean))
```

Rather than compute one feature at a time, it is convenient to compute many features at once. A common short summary of a data set is to compute five summary statistics: the minimum, first quartile, median, third quartile and maximum. These divide the data into four equal-size sections, each containing 25% of the data. The quantile() function can be used to compute them.

```{r Compute and present quantiles: Minimum, first quartile, median, third quartile, and maximum}
tourism %>% features(Trips, quantile)
```

## 4.2 Autocorrelation (ACF) Features

We discuss differencing of time series in more detail in Section 9.1.

The `feat_acf()` function computes a selection of the autocorrelations discussed here. It will return six or seven features:

• the first autocorrelation coefficient from the original data;
• the sum of squares of the first ten autocorrelation coefficients from the original data;
• the first autocorrelation coefficient from the differenced data;
• the sum of squares of the first ten autocorrelation coefficients from the differenced data;
• the first autocorrelation coefficient from the twice differenced data;
• the sum of squares of the first ten autocorrelation coefficients from the twice differenced data;
• For seasonal data, the autocorrelation coefficient at the first seasonal lag is also returned.

For example:

```{r examples of features of autocorrelations of data}
View(tourism %>% filter(Purpose == "Business") %>% features(Trips, feat_acf))
```

## 4.3 STL Features

A time series decomposition can be used to measure the strength of trend and seasonality in a time series. Recall that the decomposition is written as:

$$y_t = T_t + S_t + R_t$$

where $$T_t$$ is the smoothed trend component, $$S_t$$  is the seasonal component and  $$R_t$$ is a remainder component. For strongly trended data, the seasonally adjusted data should have much more variation than the remainder component.

We define the strength of  trend as:

$$F_T = max \left(0, 1 -\frac{Var(R_t)}{Var(T_t + R_t)}\right)$$

Strength of seasonality is similar:

$$F_S = max \left(0, 1 -\frac{Var(R_t)}{Var(S_t + R_t)}\right)$$

A series with seasonal strength $$F_S$$ close to 0 exhibits almost no seasonality, while a series with strong seasonality will have $$F_s$$ close to 1 vbecayse $$Var(R_t)$$ will be much smaller than $$Var(S_t + R_t)$$

```{r}
tourism %>% 
  features(Trips, feat_stl)
```

<b>We can then use these features in plots to identify what type of series are heavily trended and what are most seasonal.</b>

```{r Figure 4.1: Seasonal strength vs trend strength for all tourism series.}
tourism %>% 
  features(Trips, feat_stl) %>% 
  ggplot(aes(x = trend_strength, y = seasonal_strength_year, col = Purpose)) +
  geom_point() +
  facet_wrap(vars(State))
```

Clearly, holiday series are most seasonal which is unsurprising. The strongest trends tend to be in Western Australia and Victoria. The most seasonal series can also be easily identified and plotted.

```{r Figure 4.2: The most seasonal series in the Australian tourism data.}

tourism %>% 
  features(Trips, feat_stl) %>% 
  filter(seasonal_strength_year == max(seasonal_strength_year)) %>% 
  left_join(tourism, by = c("State", "Region", "Purpose")) %>% 
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(vars(State, Region, Purpose))

```

This shows holiday trips to the most popular ski region of Australia.

The `feat_stl()` function returns several more features other than those discussed above.

• `seasonal_peak_year` indicates the timing of the peaks — which month or quarter contains the largest seasonal component. This tells us something about the nature of the seasonality. In the Australian tourism data, if Quarter 3 is the peak seasonal period, then people are travelling to the region in winter, whereas a peak in Quarter 1 suggests that the region is more popular in summer.
• `seasonal_trough_year` indicates the timing of the troughs — which month or quarter contains the smallest seasonal component.
• `spikiness` measures the prevalence of spikes in the remainder component $$R_t$$ of the of the STL decomposition. It is the variance of the leave-one-out variances of $$R_t$$
• linearity measures the linearity of the trend component of the STL decomposition. It is based on the coefficient of a linear regression applied to the trend component.
• curvature measures the curvature of the trend component of the STL decomposition. It is based on the coefficient from an orthogonal quadratic regression applied to the trend component.
• `stl_e_acf1` is the first autocorrelation coefficient of the remainder series.
• `stl_e_acf10` is the sum of squares of the first ten autocorrelation coefficients of the remainder series.

## 4.4 List of other features

The remaining features in the feasts package, not previously discussed, are listed here for reference. The details of some of them are discussed later in the book.

• `coef_hurst` will calculate the Hurst coefficient of a time series which is a measure of “long memory.” A series with long memory will have significant autocorrelations for many lags.
• `feat_spectral` will compute the (Shannon) spectral entropy of a time series, which is a measure of how easy the series is to forecast. A series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0. A series that is very noisy (and so is difficult to forecast) will have entropy close to 1.
• `box_pierce` gives the Box-Pierce statistic for testing if a time series is white noise, and the corresponding p-value. This test is discussed in Section 5.4.
• `ljung_box` gives the Ljung-Box statistic for testing if a time series is white noise, and the corresponding p-value. This test is discussed in Section 5.4.
The k-th partial autocorrelation measures the relationship between observations k periods apart after removing the effects of observations between them. So the first partial autocorrelation (k=1) is identical to the first autocorrelation, because there is nothing between consecutive observations to remove. Partial autocorrelations are discussed in Section 9.5. The `feat_pacf` function contains several features involving partial autocorrelations including the sum of squares of the first five partial autocorrelations for the original series, the first-differenced series and the second-differenced series. For seasonal data, it also includes the partial autocorrelation at the first seasonal lag.
• `unitroot_kpss` gives the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) statistic for testing if a series is stationary, and the corresponding p-value. This test is discussed in Section 9.1.
• `unitroot_pp` gives the Phillips-Perron statistic for testing if a series is non-stationary, and the corresponding p-value.
• `unitroot_ndiffs` gives the number of differences required to lead to a stationary series based on the KPSS test. This is discussed in Section 9.1
unitroot_nsdiffs gives the number of seasonal differences required to make a series stationary. This is discussed in Section 9.1.
• `var_tiled_mean` gives the variances of the “tiled means” (i.e., the means of consecutive non-overlapping blocks of observations). The default tile length is either 10 (for non-seasonal data) or the length of the seasonal period. This is sometimes called the “stability” feature.
• `var_tiled_var` gives the variances of the “tiled variances” (i.e., the variances of consecutive non-overlapping blocks of observations). This is sometimes called the “lumpiness” feature.
• `shift_level_max` finds the largest mean shift between two consecutive sliding windows of the time series. This is useful for finding sudden jumps or drops in a time series.
• `shift_level_index` gives the index at which the largest mean shift occurs.
• `shift_var_max` finds the largest variance shift between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the volatility of a time series.
• `shift_var_index` gives the index at which the largest variance shift occurs.
• `shift_kl_max` finds the largest distributional shift (based on the Kulback-Leibler divergence) between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the distribution of a time series.
• `shift_kl_index` gives the index at which the largest KL shift occurs.
• `n_crossing_points` computes the number of times a time series crosses the median.
longest_flat_spot computes the number of sections of the data where the series is relatively unchanging.
stat_arch_lm returns the statistic based on the Lagrange Multiplier (LM) test of Engle (1982) for autoregressive conditional heteroscedasticity (ARCH).
• `guerrero` computes the optimal λ value for a Box-Cox transformation using the Guerrero method (discussed in Section 3.1).
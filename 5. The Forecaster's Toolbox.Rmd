---
title: "5. The Forecaster's Toolbox.RMD"
author: "Russ Conte"
date: "10/4/2021"
output: html_document
---

# 5.1 A tidy forecasting workflow

![](5. Tidy_forecasting_workflow.jpg)

## Data preparation (tidy)

We will model GDP per capita over time; so first, we must compute the relevant variable.

```{r}
library(tidyverse)
library(fpp3)
gdppc <- global_economy %>% 
  mutate(GDP_per_capita = GDP / Population)
gdppc

```

## Plot the data (visualize), we will use Sweden as our example

```{r Plot the data for Sweden}
gdppc %>% 
  filter(Country == "Sweden") %>% 
  autoplot(GDP_per_capita) +
  labs( y = "$US", title = "GDP per capita for Sweden")

TSLM(GDP_per_capita ~ trend())
```

## Definte a model (specify)
There are many different time series models that can be used for forecasting, and much of this book is dedicated to describing various models. Specifying an appropriate model for the data is essential for producing appropriate forecasts. For example, a linear trend model (to be discussed in Chapter 7) for GDP per capita can be specified with:

```{r Define the model}
TSLM(GSP_per_capita ~ trend())
```

## Train the model
Once an appropriate model is specified, we next train the model on some data. One or more model specifications can be estimated using the model() function.

To estimate the model in our example, we use

```{r Train the model}
fit <- gdppc %>%
   model(trend_model = TSLM(GDP_per_capita ~ trend()))
fit
```

This fits a linear trend model to the GDP per capita data for each combination of key variables in the tsibble. In this example, it will fit a model to each of the 263 countries in the dataset. The resulting object is a model table or a “mable.”


## Check model performance

Once a model has been fitted, it is important to check how well it has performed on the data. There are several diagnostic tools available to check model behaviour, and also accuracy measures that allow one model to be compared against another.

## Produce forecasts (forecast)

With an appropriate model specified, estimated and checked, it is time to produce the forecasts using `forecast()`. The easiest way to use this function is by specifying the number of future observations to forecast. For example, forecasts for the next 10 observations can be generated using `h = 10`. We can also use natural language; e.g., `h = "2 years"` can be used to predict two years into the future.

```{r Forecast 3 years into the future}

fit %>% forecast(h = "3 years")
fit

```


This is a forecast table, or “fable.” Each row corresponds to one forecast period for each country. The `GDP_per_capita` column contains the forecast distribution, while the .mean column contains the point forecast. The point forecast is the mean (or average) of the forecast distribution.

The forecasts can be plotted along with the historical data using `autoplot()` as follows:

```{r Figure 5.2: Forecasts of GDP per capita for Sweden using a simple trend model. }
fit %>% 
  forecast(h = "3 years") %>% 
  filter(Country == "Sweden") %>% 
  autoplot(gdppc) +
  labs(y = "$US", title = "GDP per capita for Sweden")
```

# 5.1 Some simple forecasting methods

We will use four simple forecasting methodsas benchmarks throughout this book. To illustrate them, we will use quarterly Australian clay brick production between 1970 and 2004:

```{r Clay brick production}
# bricks <- aus_production %>% 
#   filter_index("1970 Q1" ~ "2004 Q4")

bricks <- aus_production %>%
  filter_index("1970 Q1" ~ "2004 Q4")

```

The `filter_index()` function is a convenient shorthand for extracting a section of a time series.

## Mean method

Here, the forecasts of all future values are equal to the average (or “mean”) of the historical data. If we let the historical data be denoted by y1,…,yT, then we can write the forecasts as

$$\hat{y}_{T+h|T}=\bar{y}=(y_1 + ... +y_T)/T$$

```{r Figure 5.3: Mean (or average) forecasts applied to clay brick production in Australia. }

train <- aus_production %>%
  filter_index("1970 Q1" ~ "2004 Q4")
# Fit the models
bricks_fit <- train %>%
  model(
    Mean = MEAN(Bricks),
  )
# Generate forecasts for 23 quarters
bricks_fc <- bricks_fit %>% forecast(h = 23)
# Plot forecasts against actual values
bricks_fc %>%
  autoplot(train, level = NULL) +
  labs(
    y = "Bricks in thousands ('000)",
    title = "Forecasts for quarterly brick production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))


```


## Naïve method

For naïve forecasts, we simply set all forecasts to be the value of the last observation. That is,

$$\hat{y}_{T+h|T}=y_T$$

<b>This method works remarkably well for many economic and financial time series.</b>

```{r Figure 5.4: Naïve forecasts applied to clay brick production in Australia. }

train <- aus_production %>%
  filter_index("1970 Q1" ~ "2004 Q4")
# Fit the models
bricks_fit <- train %>%
  model(
    naive = NAIVE(Bricks),
  )
# Generate forecasts for 23 quarters
bricks_fc <- bricks_fit %>% forecast(h = 23)
# Plot forecasts against actual values
bricks_fc %>%
  autoplot(train, level = NULL) +
  labs(
    y = "Bricks in thousands ('000)",
    title = "Forecasts for quarterly brick production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))



```

## Seasonal naïve method

A similar method is useful for highly seasonal data. In this case, we set each forecast to be equal to the last observed value from the same season of the year (e.g., the same month of the previous year). Formally, the forecast for time T+h is written as

$$\hat{y}_{T+h|T}=y_{T+h-m(k+1)}$$

where m= the seasonal period, and k is the integer part of (h−1)/m (i.e., the number of complete years in the forecast period prior to time T+h).

```{r Figure 5.5: Seasonal naïve forecasts applied to clay brick production in Australia.}

train <- aus_production %>%
  filter_index("1970 Q1" ~ "2004 Q4")
# Fit the models
bricks_fit <- train %>%
  model(
    snaive = SNAIVE(Bricks),
  )
# Generate forecasts for 23 quarters
bricks_fc <- bricks_fit %>% forecast(h = 23)
# Plot forecasts against actual values
bricks_fc %>%
  autoplot(train, level = NULL) +
  labs(
    y = "Bricks in thousands ('000)",
    title = "Forecasts for quarterly brick production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))

```

## Drift method

A variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the <b>drift</b>) is set to be the average change seen in the historical data. Thus the forecast for time T+h is given by

$$\hat{y}_{T+h|T}=y_T + \frac{h}{T-1}\sum_{t=2}^T(y_t - y_{t-1}) = y_T + h\left( \frac{y_t - y_1}{T-1} \right)$$

This is equivalent to drawing a line between the first and last observations, and extrapolating it into the future.

```{r}
train <- aus_production %>%
  filter_index("1970 Q1" ~ "2004 Q4")
# Fit the models
bricks_fit <- train %>%
  model(
    RW(Bricks ~ drift()),
  )
# Generate forecasts for 23 quarters
bricks_fc <- bricks_fit %>% forecast(h = 23)
# Plot forecasts against actual values
bricks_fc %>%
  autoplot(train, level = NULL) +
  labs(
    y = "Bricks in thousands ('000)",
    title = "Forecasts for quarterly brick production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```

Conclusion of section 5.2

## 5.3 Fitted values and residuals

Each observation in a time series can be forecast using all previous observations. We call these <b>fitted values</b> and they are denoted by , meaning the forecast of yt based on observations y1,…,yt−1 . We use these so often, we sometimes drop part of the subscript and just write ^yt instead of ^yt|t−1. Fitted values almost always involve one-step forecasts.

For example, if we use the mean method, the fitted values are given by $$\hat{y_t} = \hat{c}$$

When the estimate of c involves observations after time t, the fitted values are not true forecasts. On the other hand, naïve or seasonal naïve forecasts do not involve any parameters, and so fitted values are true forecasts in such cases.

## Residuals

The “residuals” in a time series model are what is left over after fitting a model. The residuals are equal to the difference between the observations and the corresponding fitted values:

$$e_t = y_t - \hat{y_t}$$

The fitted values and residuals from a model can be obtained using the `augment()` function. In the beer production example in Section 5.2, we saved the fitted models as `beer_fit`. So we can simply apply `augment()` to this object to compute the fitted values and residuals for all models.

```{r}
augment(bricks_fit)
```

There are three new columns added to the original data:

•    `.fitted` contains the fitted values;
•    `.resid` contains the residuals;
•    `.innov` contains the “innovation residuals” which, in this case, are identical to the regular residuals.

## 5.4 Residual diagnostics

A good forecasting method will yield innovation residuals with the following properties:

1. The innovation residuals are uncorrelated. If there are correlations between innovation residuals, then there is information left in the residuals which should be used in computing forecasts.
2. The innovation residuals have zero mean. If they have a mean other than zero, then the forecasts are biased.

If either of these properties is not satisfied, then the forecasting method can be modified to give better forecasts. Adjusting for bias is easy: if the residuals have mean m, then simply subtract m from all forecasts and the bias problem is solved. Fixing the correlation problem is harder, and we will not address it until Chapter 10.

3. The innovation residuals have constant variance. This is known as “homoscedasticity”.
4. The innovation residuals are normally distributed.

```{r Figure 5.9: Daily Google stock prices in 2015.}
google_stock <- gafa_stock %>%
  filter(Symbol == "GOOG", year(Date) >= 2015) %>%
  mutate(day = row_number()) %>%
  update_tsibble(index = day, regular = TRUE)

google_2015 <- google_stock %>% filter(year(Date) == 2015)
# Fit the models
google_fit <- google_2015 %>%
  model(
    Mean = MEAN(Close),
    `Naïve` = NAIVE(Close),
    Drift = NAIVE(Close ~ drift())
  )
# Produce forecasts for the trading days in January 2016
google_jan_2016 <- google_stock %>%
  filter(yearmonth(Date) == yearmonth("2016 Jan"))
google_fc <- google_fit %>%
  forecast(new_data = google_jan_2016)
# Plot the forecasts
google_fc %>%
  autoplot(google_2015, level = NULL) +
  autolayer(google_jan_2016, Close, colour = "black") +
  labs(y = "$US",
       title = "Google daily closing stock prices",
       subtitle = "(Jan 2015 - Jan 2016)") +
  guides(colour = guide_legend(title = "Forecast"))


```

The residuals obtained from forecasting this series using the naïve method are shown in Figure 5.10. The large positive residual is a result of the unexpected price jump in July.

```{r Figure 5.10: Residuals from forecasting the Google stock price using the naïve method.}

aug <- google_2015 %>% 
  model(NAIVE(Close)) %>% 
  augment()
autoplot(aug, .innov) +
  labs(y = "$US",
       title = "Residuals from the naïve method")

```

```{r Figure 5.11: Histogram of the residuals from the naïve method applied to the Google stock price. }
aug %>%
  ggplot(aes(x = .innov)) +
  geom_histogram() +
  labs(title = "Histogram of residuals")
```

```{r Figure 5.12: ACF of the residuals from the naïve method applied to the Google stock price.}

aug %>%
  ACF(.innov) %>%
  autoplot() +
  labs(title = "Residuals from the naïve method")

```

These graphs show that the naïve method produces forecasts that appear to account for all available information. The mean of the residuals is close to zero and there is no significant correlation in the residuals series. The time plot of the residuals shows that the variation of the residuals stays much the same across the historical data, apart from the one outlier, and therefore the residual variance can be treated as constant. This can also be seen on the histogram of the residuals. The histogram suggests that the residuals may not be normal — the right tail seems a little too long, even when we ignore the outlier. Consequently, forecasts from this method will probably be quite good, but prediction intervals that are computed assuming a normal distribution may be inaccurate.

A convenient shortcut for producing these residual diagnostic graphs is the gg_tsresiduals() function, which will produce a time plot, ACF plot and histogram of the residuals.

```{r Figure 5.13: Residual diagnostic graphs for the naïve method applied to the Google stock price.}

google_2015 %>% 
  model(NAIVE(Close)) %>% 
  gg_tsresiduals()

```

## Portmanteau tests for autocorrelation

In addition to looking at the ACF plot, we can also do a more formal test for autocorrelation by considering a whole set of rk values as a group, rather than treating each one separately.

the <b>Ljun-Box test</b>, based on

$$Q^*=T(T+2)\sum_{k=1}^\ell(T-k)^{-1}r^2_k$$

For the Google stock price example, the naïve method has no parameters, so K=0 in that case also. In the following code, lag=ℓ and dof=K.

```{r}

aug %>% features(.innov, box_pierce, lag = 10, dof = 0)

aug %>% features(.innov, ljung_box, lag = 10, dof = 0)

```

For both Q and Q∗, the results are not significant (i.e., the p-values are relatively large). Thus, we can conclude that the residuals are not distinguishable from a white noise series.

An alternative simple approach that may be appropriate for forecasting the Google daily closing stock price is the drift method. The `tidy()` function shows the one estimated parameter, the drift coefficient, measuring the average daily change observed in the historical data.

```{r}
fit <- google_2015 %>% model(RW(Close ~ drift()))
tidy(fit)
```

Applying the Ljung-Box test, we set K=1 to account for the estimated parameter.

```{r}
augment(fit) %>% features(.innov, ljung_box, lag = 10, dof = 1)

```

As with the naïve method, the residuals from the drift method are indistinguishable from a white noise series.

Conclusion of section 5.4

# 5.5 Distribution forecasts and prediction intervals

<b><mark>If we only produce point forecasts, there is no way of telling how accurate the forecasts are. However, if we also produce prediction intervals, then it is clear how much uncertainty is associated with each forecast. For this reason, point forecasts can be of almost no value without the accompanying prediction intervals.</mark></b>

A prediction interval gives an interval within which we expect $$y_t$$ to lie within a specified probability. For example, assuming that distribution of future observations is normal, a 95% prediction interval for the h-step forecast is:

$$\hat{y}_{T+h|T}\pm 1.96\hat{\sigma}_h$$

## One-step prediction intervals

When forecasting one step ahead, the standard deviation of the forecast distrubtion can be estimated using the standard deviation of the residuals given by:

$$\hat{\sigma} = \sqrt{\frac{1}{T - K}\sum_{t=1}^Te^2_t}$$

## Multi-step prediction intervals

A common feature of prediction intervals is that they usually increase in length as the forecast horizon increases. The further ahead we forecast, the more uncertainty is associated with the forecast, and thus the wider the prediction intervals. That is, σh usually increases with h

(although there are some non-linear forecasting methods which do not have this property).

To produce a prediction interval, it is necessary to have an estimate of σh
. As already noted, for one-step forecasts (h=1), Equation (5.1) provides a good estimate of the forecast standard deviation σ1. For multi-step forecasts, a more complicated method of calculation is required. These calculations assume that the residuals are uncorrelated.

| Benchmark Method | h-step forecast standard deviation |
|--------------|-----------|
| Mean | $$\hat{\sigma}_h = \hat{\sigma}\sqrt{1 + \frac{1}{T}}$$  |
| Naïve |$$\hat{\sigma}_h = \hat{\sigma}\sqrt{h}$$  |
| Seasonal naïve | $$\hat{\sigma}_h = \hat{\sigma}\sqrt{k + 1}$$ |
| Drift | $$\hat{\sigma}_h = \hat{\sigma}\sqrt{h(1 + \frac{h}{T})}$$

Prediction intervals can easily be computed for you when using the `fable` package. For example here is the output when using the naïve method for the Google stock price:

```{r Prediction intervals for Google stock price}
google_2015 %>% 
  model(NAIVE(Close)) %>% 
  forecast(h = 10) %>% 
  hilo()


```

The `hilo()` function converts the forecast distributions into intervals. By default, 80% and 95% prediction intervals are returned, although other options are possible via the level argument. For example, here are 90% prediction intervals:

```{r 90% prediction intervals}
google_2015 %>% 
  model(NAIVE(Close)) %>% 
  forecast(h = 10) %>% 
  hilo(level = 0.90)

```

When plotted, the prediction intervals are shown as shaded regions, with the strength of colour indicating the probability associated with the interval. Again, 80% and 95% intervals are shown by default, with other options available via the level argument.

```{r}

google_2015 %>% 
  model(NAIVE(Close)) %>% 
  forecast(h = 10) %>% 
  autoplot(google_2015) + 
  labs(title="Google daily closing stock price", y = "$US")

```

## Prediction from bootstrapped residuals

A one step error is defined as:

$$e_t = y_t - \hat{y}_{t|t-1}$$
Simple algebra yields:
$$y_t = \hat{y}_{t|t-1} + e_t$$

We can simulate the next observation of a time series using:

$$y_{T+1} = \hat{y}_{T+1|t} + e_{T+1}$$

Doing this repeatedly, we obtain many possible futures. To see some of them, we can use the `generate()` function:

```{r Using the generate function to obtain five possible futures}

fit <-google_2015 %>% 
  model(NAIVE(Close))
sim <- fit %>% generate(h = 30, times = 5, bootstrap = TRUE)

```

We can plot the five possible futures, as follows:

```{r Plot the five possible futures, based on a naïve model, with bootstrapped residuals}

google_2015 %>% 
  ggplot(aes(x = day)) +
  geom_line(aes(y = Close)) +
  geom_line(aes(y = .sim, colour = as.factor(.rep)),
            data = sim) +
  labs(title = "Google daily closing stock price", y = "$US") +
  guides(colour = "none")

```

Then we can compute prediction intervals by calculating percentiles of the future sample paths for each forecast horizon. The result is called a <b>bootstrapped</b> prediction interval. The name “bootstrap” is a reference to pulling ourselves up by our bootstraps, because the process allows us to measure future uncertainty by only using the historical data.

This is all built into the `forecast()` function so you do not need to call `generate()` directly.

```{r Boostraping using forecast}

fc <- fit %>% forecast(h = 30, bootstrap = TRUE)
fc

```

Notice that the forecast distribution is now represented as a simulation with 5000 sample paths. Because there is no normality assumption, the prediction intervals are not symmetric.

```{r Figure 5.16: Forecasts of the Google closing stock price based on a naïve method with bootstrapped residuals.}

autoplot(fc, google_2015) +
  labs(title = "Google daily closing stock price", y = "$US")

```

Completion through section 5.5

## Forecasting using transformations

Some common transformations which can be used when modelling were discussed in Section 3.1. When forecasting from a model with transformations, we first produce forecasts of the transformed data. Then, we need to reverse the transformation (or back-transform) to obtain forecasts on the original scale. For Box-Cox transformations given by (3.1), the reverse transformation is given by:

$$y_{t} =
    \begin{cases}
      \exp(w_{t}) & \text{if $\lambda=0$};\\
      \text{sign}(\lambda w_t+1)|\lambda w_t+1|^{1/\lambda} & \text{otherwise}.
    \end{cases}$$
    
## Bias adjustments
One issue with using mathematical transformations such as Box-Cox transformations is that the back-transformed point forecast will not be the mean of the forecast distribution. In fact, it will usually be the median of the forecast distribution (assuming that the distribution on the transformed space is symmetric). For many purposes, this is acceptable, although the mean is usually preferable. For example, you may wish to add up sales forecasts from various regions to form a forecast for the whole country. But medians do not add up, whereas means do.

For a Box-Cos transformation, the back-transformed mean is given (approximately) by:

$$\hat{y}_{T+h|T} =
  \begin{cases}
     \exp(\hat{w}_{T+h|T})\left[1 + \frac{\sigma_h^2}{2}\right] & \text{if $\lambda=0$;}\\
     (\lambda \hat{w}_{T+h|T}+1)^{1/\lambda}\left[1 + \frac{\sigma_h^2(1-\lambda)}{2(\lambda \hat{w}_{T+h|T}+1)^{2}}\right] & \text{otherwise;}
  \end{cases} $$

The difference between the simple back-transformed forecast given by (5.2) and the mean given by (5.3) is called the <b>bias</b>. When we use the mean, rather than the median, we say the point forecasts have been <b>bias-adjusted</b>.

To see how much difference this bias-adjustment makes, consider the following example, where we forecast the average annual price of eggs using the drift method with a log transformation (λ=0). The log transformation is useful in this case to ensure the forecasts and the prediction intervals stay positive.

```{r Annual egg prices}
prices %>% 
  filter(!is.na(eggs)) %>% 
  model(RW(log(eggs) ~ drift())) %>% 
  forecast(h = 50) %>% 
  autoplot(prices %>% filter(!is.na(eggs)),
           level = 80, point_forecast = lst(mean, median)) +
  labs(title = "Annual egg prices",
       y = "$US (in centra adjusted for inflation")

```

The dashed line in Figure 5.17 shows the forecast medians while the solid line shows the forecast means. Notice how the skewed forecast distribution pulls up the forecast distribution’s mean; this is a result of the added term from the bias adjustment.

Bias-adjusted forecast means are automatically computed in the fable package. The forecast median (the point forecast prior to bias adjustment) can be obtained using the median() function on the distribution column.

Completion of section 5.6

## Forecasting with decomposition

Assuming additive decomposition, the decomposed time series can be written as:

$$y_t = \hat{S_t} + \hat{A_t} $$ 

If a multiplicative decomposition has been used, we can write:

$$y_y = \hat{S_t}\hat{A_t} $$

To forecast a decomposed time series, we forecast the seasonal component, ${S_t}$, and the seasonally adjusted component ${A_t}$, separately.

## Example: Employment in the US Retail Sector

```{r Figure 5.18: Naïve forecasts of the seasonally adjusted data}

us_retail_employment <- us_employment %>% 
  filter(year(Month) >= 1990, Title == "Retail Trade")
dcmp <- us_retail_employment %>% 
  model(STL(Employed~trend(window = 7), robust = TRUE)) %>% 
  components() %>% 
  select(-.model)
dcmp %>% 
  model(NAIVE(season_adjust)) %>% 
  forecast() %>% 
  autoplot(dcmp) +
  labs(y = "number of people",
       title = "US Retail Employment")

```

Figure 5.18 shows naïve forecasts of the seasonally adjusted US retail employment data. These are then “reseasonalised” by adding in the seasonal naïve forecasts of the seasonal component.

This is made easy with the `decomposition_model()` function, which allows you to compute forecasts via any additive decomposition, using other model functions to forecast each of the decomposition’s components. Seasonal components of the model will be forecast automatically using `SNAIVE()` if a different model isn’t specified. The function will also do the reseasonalising for you, ensuring that the resulting forecasts of the original data are obtained. These are shown in Figure 5.19.

```{r Figure 5.19: Forecasts of the total US retail employment data based on a naïve forecast of the seasonally adjusted data}

fit_dcmp <- us_retail_employment %>% 
  model(stlf = decomposition_model(
    STL(Employed ~ trend(window = 7), robust = TRUE),
    NAIVE(season_adjust)
  ))
fit_dcmp %>% 
  forecast() %>% 
  autoplot(us_retail_employment) +
  labs(y = "Number of people",
       title = "US Retail Employment")
```

The prediction intervals shown in this graph are constructed in the same way as the point forecasts. That is, the upper and lower limits of the prediction intervals on the seasonally adjusted data are “reseasonalised” by adding in the forecasts of the seasonal component.

The ACF of the residuals shown in Figure 5.20, display significant autocorrelations. These are due to the naïve method not capturing the changing trend in the seasonally adjusted series.

```{r Figure 5.20: Checking the residuals.}
fit_dcmp %>% gg_tsresiduals()

```

Completion through section 5.7

## 5.8 Evaluating point forecast accuracy

### Training and test sets

When choosing models, it is common practice to separate the available data into two portions, training and test data, where the training data is used to estimate any parameters of a forecasting method and the test data is used to evaluate its accuracy. Because the test data is not used in determining the forecasts, it should provide a reliable indication of how well the model is likely to forecast on new data.

The size of the test set is typically about 20% of the total sample, although this value depends on how long the sample is and how far ahead you want to forecast. The test set should ideally be at least as large as the maximum forecast horizon required. The following points should be noted.

•   A model which fits the training data well will not necessarily forecast well.
•   A perfect fit can always be obtained by using a model with enough parameters.
•   Over-fitting a model to data is just as bad as failing to identify a systematic pattern in the data.

### Functions to subset a time series

The filter() function is useful when extracting a portion of a time series, such as we need when creating training and test sets. When splitting data into evaluation sets, filtering the index of the data is particularly useful. For example,

```{r Subset a time series, example 1}
aus_production %>% filter(year(Quarter)>= 1995)

```

Equivalently,

```{r Equivalent way to subset a time series}
aus_production %>% filter_index("1995 Q1" ~.,)
# This is equivalent to the previous data set

```

Another useful function is slice(), which allows the use of indices to choose a subset from each group. For example,

```{r Slicing data to subset a time series}
aus_production %>% 
  slice(n()-19:0)
```

will extract the last 20 observations (five years)

Slice also works with groups, making it possible to subset observations from each key. For example:

```{r slicing by groups}
aus_retail %>% 
  group_by(State, Industry) %>% 
  slice(1:12)

```

will subset the first year of data from each time series in the data.

## Forecast errors

A forecast "error" is the difference between an observed value and its forecast. Here "error" does not mean mistake, it means the unpredictable part of an observation. It can be written as:

$$e_{T+h} = y_{T+h} - \hat{y}_{T+h|T} $$

where the training data is given by $\left\{y_1,...y_t \right\}$ and the test data is given by $\left\{ y_{T+1}, y_{T+2}, ...\right\}$

The two most commonly used scale-dependent measures are based on the absolute errors or squared errors:

Mean absolute error: MAE = mean(|$e_t|),
Root mean squared error: RMSE = $\sqrt{mean(e^2_t)}$

When comparing forecast methods applied to a single time series, or to several time series with the same units, the MAE is popular as it is easy to both understand and compute. <b><mark>A forecast method that minimises the MAE will lead to forecasts of the median, while minimising the RMSE will lead to forecasts of the mean.</mark></b>

## Percentage errors

The percentage error is given by $p_t = 100e_t/y_t$. Percentage errors have the advantage of being unit = free, so are frequently used to compare forecast performances between data sets. The most commonly used measure is:

Mean absolute percentage error: MAPE = mean(|$p_t$|)

## Scaled errors

Scaled errors were proposed by Hyndman & Koehler (2006) as an alternative to using percentage errors when comparing forecast accuracy across series with different units. They proposed scaling the errors based on the <i>training</i> MAE from a simple forecast method.

For a non-seasonal time series, a useful way to define a scaled error uses naïve forecasts:

$$q_j = \frac{e_j}{\frac{1}{T-1} \sum_{t = 2}^T |y_t - y_{t-1} |}$$

A scaled error is less than one if it arises from a better forecast than the average one-step naïve forecast computed on the training data. Conversely, it is greater than one if the forecast is worse than the average one-step naïve forecast computed on the training data

For seasonal time series, a scaled error can be defined using seasonal naïve forecasts:

$$q_j = \frac{e_j}{\frac{1}{T-m}\sum_{t = m+1}^T |y_t - y_{t-m}|}$$

The <i>mean absolute scaled error</i> is simply:

MASE = mean(|$q_j^2$|)

Similarly, the <i>root mean squared scaled error</i> is given by:

RMSSE = $\sqrt{mean(q_j^2)}$

where

$$q_j^2 = \frac{e_j^2}{\frac{1}{T-m} \sum_{t = m+1}^T (y_t - y_{t-m})^2}$$

Examples:

```{r Figure 5.21: Forecasts of Australian quarterly beer production using data up to the end of 2007.}

recent_production <- aus_production %>% 
  filter(year(Quarter) >= 1992)
beer_train <- recent_production %>% 
  filter(year(Quarter) <= 2007)

beer_fit <- beer_train %>% 
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer),
    drift = RW(Beer ~ drift())
)

beer_fc <- beer_fit %>% 
  forecast(h = 10)
beer_fc %>% 
  autoplot(
    aus_production %>% filter(year(Quarter) >= 1992),
    level = NULL
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "forecast"))

```

Figure 5.21 shows three forecast methods applied to the quarterly Australian beer production using data only to the end of 2007. The actual values for the period 2008–2010 are also shown. We compute the forecast accuracy measures for this period.

```{r accuracy}
accuracy(beer_fc, recent_production)

```

It is obvious from the graph that the seasonal naïve method is best for these data, although it can still be improved, as we will discover later. Sometimes, different accuracy measures will lead to different results as to which forecast method is best. However, in this case, all of the results point to the seasonal naïve method as the best of these three methods for this data set.

To take a non-seasonal example, consider the Google stock price. The following graph shows the closing stock prices from 2015, along with forecasts for January 2016 obtained from three different methods.

```{r Google stock prices from January 2015}
google_fit <- google_2015 %>% 
  model(
    Mean = MEAN(Close),
    `Naïve` = NAIVE(Close),
    Drift = RW(Close ~ drift())
)

google_fc <- google_fit %>% 
  forecast(google_jan_2016)

google_fc %>% 
  autoplot(bind_rows(google_2015, google_jan_2016),
    level = NULL) +
  labs(y = "$US",
       title = "Google closing stock prices from Jan 2015") +
  guides(colour = guide_legend(title = "forecast"))

```

## 5.9 Evaluating distributional forecast accuracy

## Quantile scores

Consider the Google stock price example from the previous section. Figure 5.23 shows an 80% prediction interval for the forecasts from the naïve method:

```{r}

google_fc %>% 
  filter(.model == "Naïve") %>% 
  autoplot(bind_rows(google_2015, google_jan_2016), level = 80)+
             labs(y = "$US",
                  title = "Google closing stock prices")

```

The lower limit of this prediction interval gives the 10th percentile (or 0.1 quantile) of the forecast distribution, so we would expect the actual value to lie below the lower limit about 10% of the time, and to lie above the lower limit about 90% of the time. When we compare the actual value to this percentile, we need to allow for the fact that it is more likely to be above than below.

More generally, suppose we are interested in the quantile forecast with probability $p$
at future time $t$, and let this be denoted by $f_{p,t}$. That is, we expect the observation $y_t$ to be less than $f_{p,t}$ with probability $p$. For example, the 10th percentile would be $f_{0.10,t}. If $y_t$ denotes the observation at time $t$, then the <b>Quantile Score</b> is

$$Q_{p,t} = \begin{cases}
  2(1 - p) \big(f_{p,t} - y_{t}\big), & \text{if $y_{t} < f_{p,t}$}\\
  2p \big(y_{t} - f_{p,t}\big), & \text{if $y_{t} \ge f_{p,t}$} \end{cases}$$
    
The quantile score can be interpreted like an absolute error. In fact, when p=0.5, the quantile score $Q_{0.5,t}$ is the same as the absolute error. For other values of p, the “error” ($y_t−f_{p,t}$) is weighted to take account of how likely it is to be positive or negative. If p>0.5, $Q{p,t}$ gives a heavier penalty when the observation is greater than the estimated quantile than when the observation is less than the estimated quantile. The reverse is true for p<0.5.

## Consinuous Ranked Probability Score

In the Google stock price example, we can compute the average CRPS value for all days in the test set. A CRPS value is a little like a weighted absolute error computed from the entire forecast distribution, where the weighting takes account of the probabilities.

```{r}

google_fc %>% 
  accuracy(google_stock, list(crps = CRPS))

```

## 5.10 Time series cross-validation

In the following example, we compare the accuracy obtained via time series cross-validation with the residual accuracy. The `stretch_tsibble()` function is used to create many training sets. In this example, we start with a training set of length `.init=3`, and increasing the size of successive training sets by `.step=1`.

```{r Time series cross-validation accuracy}

google_2015_tr <- google_2015 %>% 
  stretch_tsibble(.init = 3, .step = 1) %>% 
  relocate(Date, Symbol, .id)
google_2015_tr

```

The .id column provides a new key indicating the various training sets. The accuracy() function can be used to evaluate the forecasts accuracy across the training sets.

```{r Time series cross validation accuracy}

google_2015_tr %>% 
  model(RW(Close ~ drift())) %>% 
  forecast(h = 1) %>% 
  accuracy(google_2015)

# Training set accuracy
google_2015 %>% 
  model(RW(Close ~ drift())) %>% 
  accuracy()

```

Completion of chapter 5!